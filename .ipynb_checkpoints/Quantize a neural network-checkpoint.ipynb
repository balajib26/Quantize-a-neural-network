{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on https://www.tensorflow.org/performance/quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small devices like Mobile Phones and Rasberry PI have very little memory and computation power. \n",
    "\n",
    "Training neural networks is done by applying many tiny nudges to the weights, and these small increments typically need floating point precision to work (though there are research efforts to use quantized representations here too).\n",
    "\n",
    "Taking a pre-trained model and running inference is very different. One of the magical qualities of Deep Neural Networks is that they tend to cope very well with high levels of noise in their inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Quantize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for the neural connections, since there are often many millions of these in a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below image shows a Relu Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nodes and and Weights of a neural network are originally stored as 32-bit floating point numbers. The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer.The size of the files is reduced by 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cURL allows us to fetch the file from this location\n",
    "curl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\n",
    "  tar -C tensorflow/examples/label_image/data -xz\n",
    "#The Graph Transform tool is designed to work on models that are saved as GraphDef files\n",
    "#(with .pb suffix,For eg,inception_v3_2016_08_28_frozen.pb) usually in a binary protobuf format. \n",
    "#This is the low-level definition of a TensorFlow computational graph. \n",
    "#TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production\n",
    "#environments. \n",
    "bazel build tensorflow/tools/graph_transforms:transform_graph\n",
    "bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n",
    "    #This tells you from where you have to download the input graph.\n",
    "  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\n",
    "    #This gives the location of output graph i.e. quantized graph\n",
    "  --out_graph=/tmp/quantized_graph.pb \\\n",
    "  --inputs=input \\\n",
    "  --outputs=InceptionV3/Predictions/Reshape_1 \\\n",
    "    #remove_nodes and strip_unused_nodes remove the nodes and operations that are not useful in the deployed graph.\n",
    "    #fold_constraints remove older graphs that containes out-of-date information that may cause import errors.\n",
    "    #Sort_unused_nodes arranges the nodes in GraphDef file in topological order, so that the inputs of any given node are always earlier than the node itself. \n",
    "  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n",
    "    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\n",
    "    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\n",
    "    strip_unused_nodes sort_by_execution_order'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
